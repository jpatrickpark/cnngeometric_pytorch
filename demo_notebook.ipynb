{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNGeometric demo notebook\n",
    "This notebook shows how to run a trained model on a given image pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.cnn_geometric_model import CNNGeometric\n",
    "from data.pf_dataset import PFDataset\n",
    "from data.download_datasets import download_PF_willow\n",
    "from image.normalization import NormalizeImageDict, normalize_image\n",
    "from util.torch_util import BatchTensorToVars, str_to_bool\n",
    "from geotnf.transformation import GeometricTnf\n",
    "from geotnf.point_tnf import *\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import warnings\n",
    "from torchvision.transforms import Normalize\n",
    "from collections import OrderedDict\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_cnn = 'resnet101'\n",
    "\n",
    "if feature_extraction_cnn=='vgg':\n",
    "    model_aff_path = 'trained_models/best_pascal_checkpoint_adam_affine_grid_loss.pth.tar'\n",
    "    model_tps_path = 'trained_models/best_pascal_checkpoint_adam_tps_grid_loss.pth.tar'\n",
    "elif feature_extraction_cnn=='resnet101':\n",
    "    model_aff_path = 'trained_models/best_pascal_checkpoint_adam_affine_grid_loss_resnet_random.pth.tar'\n",
    "    model_tps_path = 'trained_models/best_pascal_checkpoint_adam_tps_grid_loss_resnet_random.pth.tar'   \n",
    "\n",
    "source_image_path='datasets/PF-dataset/duck(S)/060_0036.png'\n",
    "target_image_path='datasets/PF-dataset/duck(S)/060_0013.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "do_aff = not model_aff_path==''\n",
    "do_tps = not model_tps_path==''\n",
    "\n",
    "# Create model\n",
    "print('Creating CNN model...')\n",
    "if do_aff:\n",
    "    model_aff = CNNGeometric(use_cuda=use_cuda,geometric_model='affine',feature_extraction_cnn=feature_extraction_cnn)\n",
    "if do_tps:\n",
    "    model_tps = CNNGeometric(use_cuda=use_cuda,geometric_model='tps',feature_extraction_cnn=feature_extraction_cnn)\n",
    "    \n",
    "# Load trained weights\n",
    "print('Loading trained model weights...')\n",
    "if do_aff:\n",
    "    checkpoint = torch.load(model_aff_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint['state_dict'].items()])\n",
    "    model_aff.load_state_dict(checkpoint['state_dict'])\n",
    "if do_tps:\n",
    "    checkpoint = torch.load(model_tps_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint['state_dict'].items()])\n",
    "    model_tps.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsTnf = GeometricTnf(geometric_model='tps', use_cuda=use_cuda)\n",
    "affTnf = GeometricTnf(geometric_model='affine', use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizeCNN = GeometricTnf(out_h=240, out_w=240, use_cuda = False) \n",
    "normalizeTnf = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # convert to torch Variable\n",
    "    image = np.expand_dims(image.transpose((2,0,1)),0)\n",
    "    image = torch.Tensor(image.astype(np.float32)/255.0)\n",
    "    image_var = Variable(image,requires_grad=False)\n",
    "\n",
    "    # Resize image using bilinear sampling with identity affine tnf\n",
    "    image_var = resizeCNN(image_var)\n",
    "    \n",
    "    # Normalize image\n",
    "    image_var = normalize_image(image_var)\n",
    "    \n",
    "    return image_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data import download_datasets\n",
    "#download_datasets.download_PF_willow()\n",
    "import pickle\n",
    "def unpickle_from_file(file_name):\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "data_sequences_2017 = unpickle_from_file('/gpfs/data/geraslab/jp4989/data/2010_2017_data/data_sequences_0709_1453.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different original input size\n",
    "for patient in data_sequences_2017[0]:\n",
    "    if len(patient)>1 and (patient[0]['original_image_size']['L-MLO'][0] != patient[1]['original_image_size']['L-MLO'][0]):\n",
    "        exam_list = patient\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4096, 3328), (3328, 2560))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam_list[0]['original_image_size']['L-MLO'][0], exam_list[1]['original_image_size']['L-MLO'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy\n",
    "def read_image_mat(file_name):\n",
    "    data = h5py.File(file_name, 'r')\n",
    "    image = numpy.array(data['image']).T\n",
    "    data.close()\n",
    "    return image\n",
    "def flip_image(image, view, horizontal_flip, mode = 'training'):\n",
    "\n",
    "    if mode == 'training':\n",
    "        if horizontal_flip == 'NO':\n",
    "            if (view == 'R-CC') or (view == 'R-MLO'):\n",
    "                image = numpy.fliplr(image)\n",
    "        elif horizontal_flip == 'YES':\n",
    "            if (view == 'L-CC') or (view == 'L-MLO'):\n",
    "                image = numpy.fliplr(image)\n",
    "    elif mode == 'medical':\n",
    "        if horizontal_flip == 'YES':\n",
    "            image = numpy.fliplr(image)\n",
    "\n",
    "    return image\n",
    "def normalise_single_image(image):\n",
    "\n",
    "    image -= numpy.mean(image)\n",
    "    image /= numpy.maximum(numpy.std(image), 10**(-5))\n",
    "def minmax(image):\n",
    "    image_max = image.max()\n",
    "    image_min = image.min()\n",
    "    image -= image_min\n",
    "    image /= image_max - image_min\n",
    "    image *= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = '/gpfs/data/geraslab/jp4989/data/2010_2017_cropped_images_hdf5/'\n",
    "source_image = read_image_mat(data_prefix+exam_list[0][\"L-MLO\"][0]+'.hdf5')\n",
    "target_image = read_image_mat(data_prefix+exam_list[1][\"L-MLO\"][0]+'.hdf5')\n",
    "source_image = flip_image(source_image, \"L-MLO\", exam_list[0]['horizontal_flip'], 'medical')\n",
    "target_image = flip_image(target_image, \"L-MLO\", exam_list[0]['horizontal_flip'], 'medical')\n",
    "source_image = source_image.astype(np.float64)\n",
    "target_image = target_image.astype(np.float64)\n",
    "minmax(source_image)\n",
    "minmax(target_image)\n",
    "source_image = source_image.astype(np.int16)\n",
    "target_image = target_image.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = np.stack((source_image,)*3, axis=-1)\n",
    "target_image = np.stack((target_image,)*3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_image = io.imread(source_image_path)\n",
    "#target_image = io.imread(target_image_path)\n",
    "\n",
    "source_image_var = preprocess_image(source_image)\n",
    "target_image_var = preprocess_image(target_image)\n",
    "\n",
    "if use_cuda:\n",
    "    source_image_var = source_image_var.cuda()\n",
    "    target_image_var = target_image_var.cuda()\n",
    "\n",
    "batch = {'source_image': source_image_var, 'target_image':target_image_var}\n",
    "\n",
    "resizeTgt = GeometricTnf(out_h=target_image.shape[0], out_w=target_image.shape[1], use_cuda = use_cuda) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_aff:\n",
    "        model_aff.eval()\n",
    "if do_tps:\n",
    "    model_tps.eval()\n",
    "\n",
    "# Evaluate models\n",
    "if do_aff:\n",
    "    theta_aff=model_aff(batch)\n",
    "    warped_image_aff = affTnf(batch['source_image'],theta_aff.view(-1,2,3))\n",
    "\n",
    "if do_tps:\n",
    "    theta_tps=model_tps(batch)\n",
    "    warped_image_tps = tpsTnf(batch['source_image'],theta_tps)\n",
    "\n",
    "if do_aff and do_tps:\n",
    "    theta_aff_tps=model_tps({'source_image': warped_image_aff, 'target_image': batch['target_image']})        \n",
    "    warped_image_aff_tps = tpsTnf(warped_image_aff,theta_aff_tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 240, 240])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['source_image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['source_image'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_aff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_without_resizing(image):\n",
    "    # convert to torch Variable\n",
    "    image = np.expand_dims(image.transpose((2,0,1)),0)\n",
    "    image = torch.Tensor(image.astype(np.float32)/255.0)\n",
    "    image_var = Variable(image,requires_grad=False)\n",
    "    \n",
    "    # Normalize image\n",
    "    image_var = normalize_image(image_var)\n",
    "    \n",
    "    return image_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_var_without_resizing = preprocess_image_without_resizing(source_image)\n",
    "target_image_var_without_resizing = preprocess_image_without_resizing(target_image)\n",
    "\n",
    "if use_cuda:\n",
    "    source_image_var_without_resizing = source_image_var_without_resizing.cuda()\n",
    "    target_image_var_without_resizing = target_image_var_without_resizing.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_image_aff_without_resizing = affTnf(source_image_var_without_resizing,theta_aff.view(-1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_image_aff_without_resizing_np = normalize_image(resizeTgt(warped_image_aff_without_resizing),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(warped_image_aff_without_resizing_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-normalize images and convert to numpy\n",
    "if do_aff:\n",
    "    warped_image_aff_np = normalize_image(resizeTgt(warped_image_aff),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "\n",
    "if do_tps:\n",
    "    warped_image_tps_np = normalize_image(resizeTgt(warped_image_tps),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "\n",
    "if do_aff and do_tps:\n",
    "    warped_image_aff_tps_np = normalize_image(resizeTgt(warped_image_aff_tps),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_subplots = 2+int(do_aff)+int(do_tps)+int(do_aff and do_tps)\n",
    "fig, axs = plt.subplots(1,N_subplots)\n",
    "axs[0].imshow(source_image)\n",
    "axs[0].set_title('src')\n",
    "axs[1].imshow(target_image)\n",
    "axs[1].set_title('tgt')\n",
    "subplot_idx = 2\n",
    "if do_aff:\n",
    "    axs[subplot_idx].imshow(warped_image_aff_without_resizing_np)\n",
    "    axs[subplot_idx].set_title('aff')\n",
    "    subplot_idx +=1 \n",
    "if do_tps:\n",
    "    axs[subplot_idx].imshow(warped_image_tps_np)\n",
    "    axs[subplot_idx].set_title('tps')\n",
    "    subplot_idx +=1 \n",
    "if do_aff and do_tps:\n",
    "    axs[subplot_idx].imshow(warped_image_aff_tps_np)\n",
    "    axs[subplot_idx].set_title('aff+tps')\n",
    "\n",
    "for i in range(N_subplots):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "fig.set_dpi(150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
